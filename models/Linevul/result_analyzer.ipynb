{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score,roc_curve, auc, confusion_matrix,classification_report\n",
    "\n",
    "def read_answers(filename):\n",
    "    answers={}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            js=json.loads(line)\n",
    "            answers[js['idx']]=js['target']\n",
    "    return answers\n",
    "\n",
    "def read_predictions(filename):\n",
    "    predictions={}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            idx,label=line.split()\n",
    "            predictions[int(idx)]=int(label)\n",
    "    return predictions\n",
    "\n",
    "def read_predictions_prob(filename):\n",
    "    predictions_prob={}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            idx,label=line.split()\n",
    "            predictions_prob[int(idx)]= float(label)\n",
    "    return predictions_prob\n",
    "\n",
    "def calculate_scores(answers,predictions,predictions_prob):\n",
    "    Acc=[]\n",
    "    Ans=[]\n",
    "    Pred=[]\n",
    "    Pred_prob=[]\n",
    "    for key in answers:\n",
    "        Ans.append(answers[key])\n",
    "        if key not in predictions:\n",
    "            logging.error(\"Missing prediction for index {}.\".format(key))\n",
    "            sys.exit()\n",
    "        Acc.append(answers[key]==predictions[key])\n",
    "    for key in predictions:\n",
    "        Pred.append(predictions[key])\n",
    "    for key in predictions_prob:\n",
    "        Pred_prob.append(predictions_prob[key])\n",
    "    scores={}\n",
    "    results = []\n",
    "#     scores['acc']=np.mean(Acc)\n",
    "    fpr, tpr, _ = roc_curve(Ans, Pred_prob)\n",
    "    results.append(auc(fpr, tpr)*100)\n",
    "    results.append(accuracy_score(Ans,Pred)*100)\n",
    "    results.append(precision_score(Ans,Pred,zero_division=0)*100)\n",
    "    results.append(recall_score(Ans,Pred)*100)\n",
    "    results.append(f1_score(Ans,Pred,zero_division=0)*100)\n",
    "    zipped_result = zip(Ans,Pred,Pred_prob)\n",
    "    sorted_zip = sorted(zipped_result, key=lambda x: x[2],reverse=True)\n",
    "    print(confusion_matrix(Ans,Pred))\n",
    "#     print('auc\\t',auc(fpr, tpr))\n",
    "#     print('acc\\t',accuracy_score(Ans,Pred))\n",
    "#     print('f1\\t',f1_score(Ans,Pred))\n",
    "#     print('recall\\t',recall_score(Ans,Pred))\n",
    "#     print('precision\\t',precision_score(Ans,Pred))\n",
    "    print(\"AUC, ACCURACY, PRECISION, RECALL, F1\")\n",
    "    print(results)\n",
    "    print(\"END of calculate scores\")\n",
    "    return results,sorted_zip,Pred_prob\n",
    "\n",
    "def read_output(test_dir,result_dir):\n",
    "    answers=read_answers(test_dir) #+'test.jsonl'\n",
    "    predictions=read_predictions(result_dir + 'predictions.txt')\n",
    "    predictions_prob = read_predictions_prob(result_dir + 'predictions_prob.txt')\n",
    "    scores,sorted_zip, Pred_prob=calculate_scores(answers,predictions,predictions_prob)\n",
    "    return scores,sorted_zip,Pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BIGVul Testing set\n",
      "linevul/origin/saved_models_llm_44/\n",
      "[[7918 9867]\n",
      " [ 349  730]]\n",
      "AUC, ACCURACY, PRECISION, RECALL, F1\n",
      "[56.388989273848935, 45.84393553859203, 6.888742096819854, 67.65523632993512, 12.504282288454949]\n",
      "END of calculate scores\n"
     ]
    }
   ],
   "source": [
    "# MUST SELECT BEST MODEL and RUN TEST BEFORE RUNNING THIS\n",
    "\n",
    "result_list = [] \n",
    "test_dir = \"$DIRECTORY_TO_BIGVUL_TEST_SET_IN_JSONL_FORMAT e.g ./bigvul_test.jsonl\"\n",
    "for i in range(44,45):\n",
    "    try:\n",
    "        result_dir = f'linevul/origin/saved_models_llm_{i}/'\n",
    "        print(result_dir)\n",
    "        result, sorted_zip, pred_prob = read_output(test_dir,result_dir)\n",
    "    except Exception as e:\n",
    "        print('error',i)\n",
    "        print(e)\n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REVEL TESTING SET\n",
      "linevul/origin/saved_models_llm_44/\n",
      "[[16674  2719]\n",
      " [  747   354]]\n",
      "AUC, ACCURACY, PRECISION, RECALL, F1\n",
      "[64.40774977422166, 83.08773299502293, 11.519687601692159, 32.15258855585831, 16.962146621945376]\n",
      "END of calculate scores\n"
     ]
    }
   ],
   "source": [
    "# MUST SELECT BEST MODEL and RUN TEST BEFORE RUNNING THIS\n",
    "print(\"REVEL TESTING SET\")\n",
    "result_list = [] \n",
    "test_dir = \"$DIRECTORY_TO_REVEAL_TRANSFORMED_INTO_JSONL -> example: ../reveal.jsonl\"\n",
    "for i in range(44,45): # experiment numbers ranges -> 44 was Extension with out Retriever\n",
    "    try:\n",
    "        result_dir = f'linevul/origin/saved_models_llm_{i}/'\n",
    "        print(result_dir)\n",
    "        result, sorted_zip, pred_prob = read_output(test_dir,result_dir)\n",
    "    except Exception as e:\n",
    "        print('error',i)\n",
    "        print(e)\n",
    "       \n",
    "        # break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
