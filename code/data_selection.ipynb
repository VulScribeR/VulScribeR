{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Clean items selectin (Ref[43] in Vulgen paper) random selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "clean_files = glob(\"./container_data/9_projects/*/Non_vulnerable_functions/*\")# Extra clean files from Vulgen's [43]\n",
    "!rm -r container_data/train/\n",
    "!mkdir container_data/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(clean_files)\n",
    "\n",
    "# selected_files = clean_files[:2233] # for 2K \n",
    "selected_files = clean_files[:5583] # for 5k\n",
    "# selected_files = clean_files[:11166] # for 10k\n",
    "# selected_files = clean_files[:16749] # 15K 16792 for 15038 items of original VGX \n",
    "# selected_files = clean_files[:22332] # for 20K\n",
    "\n",
    "destination_directory = \"./container_data/train/\"\n",
    "# Copy files to the destination directory with renaming\n",
    "for file_path in selected_files:\n",
    "    # Extract the filename\n",
    "    file_name = os.path.basename(file_path)\n",
    "    # Change the filename\n",
    "    new_file_name = file_name.replace(\".c\", \"_0.c\")\n",
    "    # Construct the destination path\n",
    "    destination_path = os.path.join(destination_directory, new_file_name)\n",
    "    # Copy the file to the destination directory with the new name\n",
    "    shutil.copyfile(file_path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM_Generated Random Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, shutil\n",
    "# VGX Sampling or Vulgen\n",
    "def sample_vgx_and_copy(count=5000, seed=42):\n",
    "    src_dir = './container_data/balanced_sampled_vgx/wild_funcs_gen_linevul/'\n",
    "    dest_dir = './container_data/train/'\n",
    "    # Get all files in the source directory\n",
    "    files = glob(os.path.join(src_dir, '*.c'))\n",
    "    print(f'count of all files: {len(files)}')\n",
    "    # Filter files that end with '_1.c'\n",
    "    filtered_files = [file for file in files if file.endswith('_1.c')]\n",
    "    print(f\"Total VGX's 10%: {len(filtered_files)}\")\n",
    "    # Seed the random number generator for reproducibility\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Randomly select the specified number of files\n",
    "    selected_files = random.sample(filtered_files, min(count, len(filtered_files)))\n",
    "    print(f\"Selected count: {len(selected_files)}\")    \n",
    "    # Copy selected files to the destination directory\n",
    "    for file in selected_files:\n",
    "        shutil.copy(file, dest_dir)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def sample_vulgen_and_copy(count=5000, seed=42):\n",
    "    vulgen = pd.read_csv(\"container_data/balanced_sampled_vulgen/wild_funcs_linevul_vulgen_comp2x.csv\")\n",
    "\n",
    "    vulgen = vulgen[vulgen['target'] == 1]\n",
    "    vulgen = vulgen[vulgen['processed_func'].astype(str).apply(lambda x:  len(x) > 10)] # delete the noise, an empty functin is at least 10 characters: void f(){} this also covers NaN values in the dataset\n",
    "    print(vulgen.value_counts())\n",
    "\n",
    "    sampled = vulgen.sample(count,random_state=seed)\n",
    "    output_directory = \"./container_data/train/\"\n",
    "    for index, row in sampled.iterrows():\n",
    "        code = row['processed_func']\n",
    "        file_name = f\"vulgen_from_vgx_paper_{index}_1.c\"\n",
    "        file_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "        with open(file_path, \"w\") as file:\n",
    "            file.write(code)  \n",
    "\n",
    "# sample_vulgen_and_copy(5000)\n",
    "# sample_vgx_and_copy(count=5000)\n",
    "# sample_vgx_and_copy(count=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Oversampling\n",
    "import random\n",
    "import os\n",
    "def random_oversample(file_list, target_size):\n",
    "    \"\"\"\n",
    "    Randomly oversample a list of filenames to reach the target size.\n",
    "\n",
    "    :param file_list: List of filenames to oversample.\n",
    "    :param target_size: Desired size of the oversampled list. (can be used for sampling as well)\n",
    "    :return: Oversampled list of filenames.\n",
    "    \"\"\"\n",
    "    if not file_list:\n",
    "        raise ValueError(\"The file list is empty.\")\n",
    "    \n",
    "    oversampled_list = []\n",
    "    \n",
    "    while len(oversampled_list) < target_size:\n",
    "        oversampled_list.append(random.choice(file_list))\n",
    "    \n",
    "    return oversampled_list\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "vuls = glob('container_data/train_devign/*1.c')\n",
    "to_oversample = random_oversample(vuls, 15000)\n",
    "output_directory = 'container_data/train'\n",
    "# for indx, file in enumerate(to_oversample):\n",
    "#     content = None\n",
    "#     with open(file, 'r') as f:\n",
    "#         content = f.read()\n",
    "#     original_file_name = os.path.basename(file)\n",
    "#     new_file_name = f'ROS_{indx}_{original_file_name}'\n",
    "#     new_file_path = os.path.join(output_directory, new_file_name)\n",
    "#     with open(new_file_path, 'w') as f:\n",
    "#         f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: vul          5618\n",
      "vul_lines    5618\n",
      "clean        5618\n",
      "generated    5618\n",
      "dtype: int64\n",
      "16: vul          5594\n",
      "vul_lines    5594\n",
      "clean        5594\n",
      "generated    5594\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Selectinng our data (from raw generated jsonl - not filtered yet!)\n",
    "gen_pd16 = pd.read_json(\"./generated/mutation_codeqwen.jsonl\", orient=\"records\", lines=True)\n",
    "\n",
    "df16 = gen_pd16.dropna(subset=['generated'])\n",
    "print(f\"16: {df16.count()}\")\n",
    "df16 = df16[df16['generated'] != \"\"]\n",
    "print(f\"16: {df16.count()}\") # None Empty ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification Phase (Parsing and Throwing out data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Parsing results for all items\n",
    "import os\n",
    "\n",
    "sample_df16 = df16\n",
    "\n",
    "!rm ./joern/joern-results.txt\n",
    "!rm -r ./joern/generated/\n",
    "!rm -r ./joern/parsed\n",
    "!mkdir ./joern/generated/\n",
    "\n",
    "output_directory = \"./joern/generated/\"\n",
    "\n",
    "for index, row in sample_df16.iterrows():\n",
    "\n",
    "    generated_content = row['generated']\n",
    "    file_name = f\"generated16_{index}_1.c\"\n",
    "    \n",
    "    file_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(generated_content.replace(\"/~/\",\"\"))  # Gnereated content may include our separator, so we remove it\n",
    "\n",
    "%cd joern/\n",
    "!./joern-parse generated/ >  joern-results.txt 2>&1\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting 5000 items out of 5496 valid items. (Original Size of Non Empties: 5594)\n"
     ]
    }
   ],
   "source": [
    "# filter using joern's parsing results and copy them to the train set\n",
    "\n",
    "output_directory = \"./container_data/train/\"\n",
    "\n",
    "def read_file_to_list(file_path):\n",
    "    lines = []\n",
    "    last = None\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            if last != None and \"generated\" not in line and \"generated\" not in last: # generated more than one error line\n",
    "                # print(\"duplicate error\") \n",
    "                continue\n",
    "            lines.append(line.strip())  # strip() removes any leading/trailing whitespace, including newline characters\n",
    "            last = line.strip()\n",
    "    return lines\n",
    "\n",
    "joern_results =  read_file_to_list(\"./joern/joern-results.txt\")\n",
    "filtered = []\n",
    "\n",
    "for i in range(len(joern_results)):\n",
    "    filtered.append(joern_results[i])\n",
    "    if not joern_results[i].startswith(\"generated\"):\n",
    "        filtered.pop()\n",
    "        filtered.pop()\n",
    "    else:\n",
    "        full_name = filtered.pop()\n",
    "        filtered.append(full_name[len(\"generated/\"):])\n",
    "\n",
    "max = 5000 # specify the number of items to be added!\n",
    "print(f\"Selecting {max} items out of {len(filtered)} valid items. (Original Size of Non Empties: {sample_df16.shape[0]})\")\n",
    "filter = True\n",
    "i = 0\n",
    "for index, row in sample_df16.iterrows():\n",
    "    if i >= max:\n",
    "        break\n",
    "    i+=1\n",
    "    generated_content = row['generated']\n",
    "    file_name = f\"generated16_{index}_1.c\"\n",
    "    \n",
    "    if filter and not file_name in filtered:\n",
    "        # print(f\"{i}\")\n",
    "        i-=1\n",
    "        continue\n",
    "    file_path = os.path.join(output_directory, file_name)\n",
    "\n",
    "    with open(file_path, \"w\") as file:\n",
    "        file.write(generated_content.replace(\"/~/\",\"\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collection of Training data {[43] cleans, devign, and the generated items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls container_data/train/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the orginal Devign Dataset\n",
    "!for devign_file in container_data/train_devign/*; do cp \"$devign_file\" container_data/train/; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSONL conversion for LineVul and Entropy (Diversity) calculation script\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the directory containing the files\n",
    "# directory = './container_data/test_separated/reveal_data/'\n",
    "directory = './container_data/train/'\n",
    "# Initialize an empty list to store the data\n",
    "sampled = []\n",
    "counter = 0\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    counter +=1\n",
    "    if filename.endswith(\".c\"):\n",
    "        # Get the full path to the file\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        try:\n",
    "        # Read the content of the file\n",
    "            with open(filepath, 'r') as file:\n",
    "                content = file.read()\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(e)\n",
    "            print(counter)\n",
    "        # Extract the target from the first of the last three characters\n",
    "        target = filename[-3]\n",
    "        \n",
    "        # Extract the index (filename without the last three characters and .c)\n",
    "        # idx = int(filename[:-4])  # Remove last 3 characters + .c\n",
    "        idx = filename[:-4]\n",
    "        # Append the data to the list\n",
    "        sampled.append({'func': content, 'idx': idx, 'target': int(target)})\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(sampled, columns=['func', 'idx', 'target'])\n",
    "\n",
    "# Display the DataFrame\n",
    "# print(df)\n",
    "\n",
    "FILE_NAME=\"vulgen_vuls5k\"\n",
    "df.to_json(f'./container_data/{FILE_NAME}.jsonl',orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls container_data/train/ | grep '_0.c$' | wc -l\n",
    "%ls container_data/train/ | grep '_1.c$' | wc -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items to be added to clean ones from [43]: 16749.62852897474\n",
      "all train files: 27792\n",
      "all vulnerable train files: 15768\n",
      "all clean train files: 12024\n",
      "Train Ratio 0.7625570776255708\n"
     ]
    }
   ],
   "source": [
    "from glob import glob \n",
    "# VULGEN adds 963 -> ratio = clean + x / (vul + 963) = 12024 + x / (10768 + 963) -> x = ratio * (10768 + 963) - 12024 = 1075.3 \n",
    "SAMPLED = 5000*3\n",
    "print(f\"Items to be added to clean ones from [43]: {1.1166419019316494 * (10768 + SAMPLED) - 12024}\") \n",
    "\n",
    "all_train_files = glob(\"./container_data/train/*.c\")\n",
    "v_train_files = glob(\"./container_data/train/*_1.c\")\n",
    "c_train_files = glob(\"./container_data/train/*_0.c\")\n",
    "\n",
    "print(f\"all train files: {len(all_train_files)}\")\n",
    "print(f\"all vulnerable train files: {len(v_train_files)}\")\n",
    "print(f\"all clean train files: {len(c_train_files)}\")\n",
    "\n",
    "print(f\"Train Ratio {len(c_train_files)/len(v_train_files)}\") # devign clean/vul 1.1166419019316494"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigVul "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "BIGVUL_TRAIN = \"../bigvul-cleaned/clean_train.csv\"\n",
    "\n",
    "\n",
    "dftr = pd.read_csv(BIGVUL_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for easier reading in RAG\n",
    "dftr[['index','target', 'processed_func','func_after', 'flaw_line']].to_json(path_or_buf=\"./container_data/bigvul-train.jsonl\",orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['index', 'Access Gained', 'Attack Origin',\n",
       "       'Authentication Required', 'Availability', 'CVE ID', 'CVE Page',\n",
       "       'CWE ID', 'Complexity', 'Confidentiality', 'Integrity',\n",
       "       'Known Exploits', 'Publish Date', 'Score', 'Summary',\n",
       "       'Update Date', 'Vulnerability Classification', 'add_lines',\n",
       "       'codeLink', 'commit_id', 'commit_message', 'del_lines',\n",
       "       'file_name', 'files_changed', 'func_after', 'func_before', 'lang',\n",
       "       'lines_after', 'lines_before', 'parentID', 'patch', 'project',\n",
       "       'project_after', 'project_before', 'target', 'vul_func_with_fix',\n",
       "       'processed_func', 'flaw_line', 'flaw_line_index',\n",
       "       'clean_processed_func'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftr.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>target</th>\n",
       "      <th>processed_func</th>\n",
       "      <th>func_before</th>\n",
       "      <th>func_after</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>186765</td>\n",
       "      <td>1</td>\n",
       "      <td>BrowserContext* SharedWorkerDevToolsAgentHost:...</td>\n",
       "      <td>BrowserContext* SharedWorkerDevToolsAgentHost...</td>\n",
       "      <td>BrowserContext* SharedWorkerDevToolsAgentHost...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171392</td>\n",
       "      <td>0</td>\n",
       "      <td>asocket* create_remote_socket(unsigned id, atr...</td>\n",
       "      <td>asocket* create_remote_socket(unsigned id, atr...</td>\n",
       "      <td>asocket* create_remote_socket(unsigned id, atr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>71810</td>\n",
       "      <td>0</td>\n",
       "      <td>static void draw_under_color_string(DrawingWan...</td>\n",
       "      <td>static void draw_under_color_string(DrawingWan...</td>\n",
       "      <td>static void draw_under_color_string(DrawingWan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141681</td>\n",
       "      <td>0</td>\n",
       "      <td>static bool isCommandLineAPIGetter(const Strin...</td>\n",
       "      <td>static bool isCommandLineAPIGetter(const Strin...</td>\n",
       "      <td>static bool isCommandLineAPIGetter(const Strin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>172977</td>\n",
       "      <td>0</td>\n",
       "      <td>static void rpng2_x_display_row(ulg row)\\n{\\n ...</td>\n",
       "      <td>static void rpng2_x_display_row(ulg row)\\n{\\n ...</td>\n",
       "      <td>static void rpng2_x_display_row(ulg row)\\n{\\n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index  target                                     processed_func  \\\n",
       "0  186765       1  BrowserContext* SharedWorkerDevToolsAgentHost:...   \n",
       "1  171392       0  asocket* create_remote_socket(unsigned id, atr...   \n",
       "2   71810       0  static void draw_under_color_string(DrawingWan...   \n",
       "3  141681       0  static bool isCommandLineAPIGetter(const Strin...   \n",
       "4  172977       0  static void rpng2_x_display_row(ulg row)\\n{\\n ...   \n",
       "\n",
       "                                         func_before  \\\n",
       "0   BrowserContext* SharedWorkerDevToolsAgentHost...   \n",
       "1  asocket* create_remote_socket(unsigned id, atr...   \n",
       "2  static void draw_under_color_string(DrawingWan...   \n",
       "3  static bool isCommandLineAPIGetter(const Strin...   \n",
       "4  static void rpng2_x_display_row(ulg row)\\n{\\n ...   \n",
       "\n",
       "                                          func_after  \n",
       "0   BrowserContext* SharedWorkerDevToolsAgentHost...  \n",
       "1  asocket* create_remote_socket(unsigned id, atr...  \n",
       "2  static void draw_under_color_string(DrawingWan...  \n",
       "3  static bool isCommandLineAPIGetter(const Strin...  \n",
       "4  static void rpng2_x_display_row(ulg row)\\n{\\n ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftr[['index','target', 'processed_func', 'func_before', 'func_after']].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
